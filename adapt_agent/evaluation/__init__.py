"""Evaluation frameworks for LLM agents."""

from typing import Any, Callable, Dict, List, Optional


class AgentEvaluator:
    """Evaluates LLM agent performance and behavior.
    
    Provides comprehensive evaluation metrics and benchmarking
    capabilities for agent quality assessment.
    """
    
    def __init__(self):
        """Initialize the AgentEvaluator."""
        self._evaluation_results: List[Dict[str, Any]] = []
        self._custom_metrics: Dict[str, Callable] = {}
    
    def register_metric(
        self,
        name: str,
        metric_func: Callable[[Any, Any], float],
    ) -> None:
        """Register a custom evaluation metric.
        
        Args:
            name: Name of the metric
            metric_func: Function that computes the metric score
        """
        self._custom_metrics[name] = metric_func
    
    def evaluate_response(
        self,
        agent_id: str,
        input_data: Any,
        output_data: Any,
        expected_output: Optional[Any] = None,
    ) -> Dict[str, Any]:
        """Evaluate an agent's response.
        
        Args:
            agent_id: Unique identifier for the agent
            input_data: Input provided to agent
            output_data: Output generated by agent
            expected_output: Expected output (if available)
            
        Returns:
            Evaluation results
        """
        results = {
            "agent_id": agent_id,
            "input": input_data,
            "output": output_data,
            "expected": expected_output,
            "metrics": {},
        }
        
        # Apply custom metrics
        for metric_name, metric_func in self._custom_metrics.items():
            try:
                score = metric_func(output_data, expected_output)
                results["metrics"][metric_name] = score
            except Exception as e:
                print(f"Error computing metric {metric_name}: {e}")
                results["metrics"][metric_name] = None
        
        self._evaluation_results.append(results)
        return results
    
    def compute_aggregate_metrics(
        self,
        agent_id: Optional[str] = None,
    ) -> Dict[str, float]:
        """Compute aggregate metrics across evaluations.
        
        Args:
            agent_id: Optional agent ID to filter results
            
        Returns:
            Dictionary of aggregate metric scores
        """
        results = self._evaluation_results
        
        if agent_id:
            results = [r for r in results if r["agent_id"] == agent_id]
        
        if not results:
            return {}
        
        aggregates = {}
        
        # Get all metric names
        metric_names = set()
        for result in results:
            metric_names.update(result["metrics"].keys())
        
        # Compute average for each metric
        for metric_name in metric_names:
            scores = [
                r["metrics"][metric_name]
                for r in results
                if metric_name in r["metrics"] and r["metrics"][metric_name] is not None
            ]
            
            if scores:
                aggregates[metric_name] = sum(scores) / len(scores)
        
        return aggregates
    
    def get_evaluation_results(
        self,
        agent_id: Optional[str] = None,
        limit: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """Get evaluation results.
        
        Args:
            agent_id: Filter by agent ID
            limit: Maximum number of results to return
            
        Returns:
            List of evaluation results
        """
        results = self._evaluation_results
        
        if agent_id:
            results = [r for r in results if r["agent_id"] == agent_id]
        
        if limit:
            results = results[-limit:]
        
        return results


__all__ = ["AgentEvaluator"]
